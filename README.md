
# Research Paper Q&A (RAG) â€” Streamlit + LangChain + Groq + FAISS

A simple **Retrieval-Augmented Generation (RAG)** app built with **Streamlit** that lets you ask questions from a folder of PDF research papers.  
It loads PDFs, chunks the text, creates embeddings using **Ollama**, stores them in **FAISS**, and answers queries using **Groq LLM**.

---

## Features

- ğŸ“„ Load all PDFs from `research_papers/`
- âœ‚ï¸ Split PDF text into chunks (RecursiveCharacterTextSplitter)
- ğŸ§  Create embeddings with **Ollama** (`nomic-embed-text`)
- ğŸ” Store and search vectors using **FAISS**
- ğŸ¤– Generate answers using **Groq** (`llama-3.1-8b-instant`)
- ğŸ§¾ Shows retrieved chunks in an expandable section

---

## Project Structure

```

your-project/
â”‚â”€â”€ app.py
â”‚â”€â”€ .env
â”‚â”€â”€ research_papers/
â”‚    â”œâ”€â”€ paper1.pdf
â”‚    â”œâ”€â”€ paper2.pdf
â”‚    â””â”€â”€ ...
â”‚â”€â”€ README.md

````

---

## Requirements

- Python 3.9+
- Ollama installed and running locally
- Groq API key

### Python Packages

Install dependencies:

```bash
pip install -r requirements.txt
````

If you donâ€™t have a `requirements.txt`, install manually:

```bash
pip install streamlit python-dotenv langchain langchain-community langchain-groq faiss-cpu pypdf
```

---

## Setup

### 1) Create `.env`

Create a `.env` file in the root folder:

```env
GROQ_API_KEY=your_groq_api_key_here
```

### 2) Put PDFs in the folder

Create a folder named `research_papers` and add your PDFs there:

```bash
mkdir research_papers
# add PDFs inside this folder
```

### 3) Start Ollama (Embeddings)

Make sure Ollama is installed and running.

Pull the embedding model:

```bash
ollama pull nomic-embed-text
```

Check Ollama is working:

```bash
ollama --version
```

---

## Run the App

```bash
streamlit run app.py
```

Open the URL shown in the terminal (usually `http://localhost:8501`).

---

## How to Use

1. Click **Document Embedding**

   * Loads PDFs from `research_papers/`
   * Splits them into chunks
   * Creates embeddings using Ollama
   * Builds a FAISS vector database

2. Type your question in the input box

3. The app retrieves top similar chunks (`k=4`) and uses Groq LLM to answer

4. Expand **Document similarity search** to view retrieved chunks

---

## Notes / Common Issues

### âœ… â€œNo PDFs found in research_papers/â€

* Ensure the folder exists and contains `.pdf` files:

  ```bash
  ls research_papers
  ```

### âœ… Ollama Connection Error (localhost:11434 refused)

* Start Ollama service and retry:

  * On Mac/Linux: open Ollama app or run it from terminal
* Make sure model exists:

  ```bash
  ollama list
  ```

### âœ… â€œI don't know based on the provided documents.â€

This happens when the retriever canâ€™t find relevant chunks.
Try improving retrieval:

* Increase `k`:

  ```python
  retriever = st.session_state.vectors.as_retriever(search_kwargs={"k": 8})
  ```
* Use better PDF loader (some PDFs extract text poorly)

---

## Tech Stack

* **Streamlit** â€” UI
* **LangChain** â€” RAG pipeline
* **Groq** â€” LLM inference (`llama-3.1-8b-instant`)
* **OllamaEmbeddings** â€” vector embeddings (`nomic-embed-text`)
* **FAISS** â€” vector store
* **PyPDFDirectoryLoader** â€” PDF loading

